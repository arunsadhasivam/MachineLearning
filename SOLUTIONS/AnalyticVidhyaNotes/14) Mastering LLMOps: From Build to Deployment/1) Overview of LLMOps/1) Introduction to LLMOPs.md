Introduction to LLMOPs:
==========================

![image](https://github.com/user-attachments/assets/0f88bf60-7c82-4a8c-b155-f2ddcc5468da)


Challenges faced by Generative AI Engineer:
=============================================

  - As genAI scientist, developer we want to work with multiple foundation models
  - we want the prompt engineer to get the best output of these models.
  - if needed, we want to train with high quality dataset that we generated and fine tuning on our task at hand.
  - we might also need to give access to internal data, thereby we have RAG applications , by which we do so
    and use the foundation models to see data that they have not seen before.


As AI/ML Engineer:
===================

![image](https://github.com/user-attachments/assets/4a4614f7-e774-4ad4-8c77-21f06ad820f1)

  - As AI/ML Engineer , working on genAI applications, they might have to work with multiple LLM third party api's
  - Also because these models are very huge, multiple GPU traning is also something they have to work with.
  - these RAG applications work on vector databses to do the similarity search.
  - **and then may be cases where  a lot of Vector Databases have to be managed**
  - these models 
      1)if fine tuned needs to be deployed and scaled on the infrastructure itself.

![image](https://github.com/user-attachments/assets/5e558625-8594-4693-ae65-5100853952ac)

  - Also at the same time,once the models are deployed one has to track and monitor all the models in production.
  - Now, these are the challenges in having LLM projects.
  - All the things that we discussed was a single LLM project ranging **multiple experiments and multiple**
     **things** that has to be done.


Why LLM Ops:
============

![image](https://github.com/user-attachments/assets/00265a0e-6c23-4928-ae8c-a1f603067bda)

  - Now scalling to numerous project can become a bottleneck and soon we will face issues into managing
    so many infrastructure components . thats where **llm Ops Come into picture**.
  - So , they are processes that can be automated using LLM Ops and thus preventing **repetitive tasks in human errors**
    that might arise when somebody is going to do all of this manually. 
  - Security and other best practices can also be incorporated into this system, because just need to create it once.
  - and then be automatically be deployed and then they would **automatically be deployed** having all the **security and
    best practices**  that we have put in to the system.

What is LLMOps:
=================

![image](https://github.com/user-attachments/assets/ec622e77-743e-40ea-b67d-d00da59e67de)


  - these are practices and processes involved in managing and operating llm(LLM)
  - so LLM Ops is this practice by which we can automate all of the ML lifecycle and can better ease out the
    efficiency of the company.
  - so, LLM Ops has lot of components in it.
  - we would be discussing multiple components.
![image](https://github.com/user-attachments/assets/59a48b4e-a96e-4d95-a149-b518db64d081)


1.PromptEngineering:
====================
 - these models work with a lot of different prompt, and managing them is also a challenge.


2.DataManagement:
===================
 - data management is also comes in to play.
 - we have internal company data which needs to be managed properly , this may include user data
   or any other sensitive information.

3.Model Management:
===================
  - because these models are huge that needs to be stored and versioned properly.

4.Database Management:
=======================

  - includes vector database.

5.Deployment and scalling:
==========================

  - All these models are needed to be deployed and scalled properly which something llmOps does very well.

6.Security and Guardrails:
==========================

  - These llm models when deployed , needs to be secured and at the sametimes guardtrails have to be
    put in place . so that the output generated by these models are not toxic in nature.

7.Monitor and observability:
============================

  - it is key aspect, we have to make sure all the infrastructure run smoothly .

Course Overview:
=================

  - 8 modules
  - Module1:.overview of llm ops.
  - Module2:major aws services(module 2 aws sagemaker in which we install sage maker notebooks and setup working env and experiments)
  - Module3:langchain (module3)
  - Module4:LangFuse ( monitor the llm applications we have good llm platform opensource)
  - Module5:sagemaker (pipeline and get used to using aws sagemaker which is a full fledged aws platform offered by amazon)
            putting these modules, training and deploying them
  - Module6:which works on continuous deployment,
            1)where we get to know about continuous deployment,
            2)what it is?,
            3) what are  different deployment types which exist to better deploy our model to production.  

  - Module7:we learn about kubernetes which is container orchesteration platform and used to deploy our LLM applications.
  - Module8: we will do load testing which is a form of real world testing which we create virtual users and used to
             load test the application bombared with lot of traffic to see how applications perform in real world or actually perform
             with live users.

  - These are the whole LLM Ops course is about.
    
  
