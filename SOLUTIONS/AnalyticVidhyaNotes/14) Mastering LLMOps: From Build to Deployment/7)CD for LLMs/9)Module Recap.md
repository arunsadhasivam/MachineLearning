Module Recap:
==============

  - define continuous deploymnent, what does it takes, different stages
  - different versioning, semantic versioning.
  - how to create a continuous deploymennt pipeline end to end.
  - discuss about key metrics for llm inference - prefill phase,decode phase , time till first token, TPS and metrics
    which i used.
  - finally compare and gauge the performance of given llm.
  - llm optimization techniques such as KV cache, pagination and different types of quantization techniques.
    all of them really helps to improve llm.
  - different types of model deployment , not just in terms of aws , but for any other cloud provider as well.
  - what different techniques and tools to deploy these models.
  - deeper in to sagemaker based endpoints, provisioning synchronous, asynchronous, serverless
  - overview of sagemaker projects and what are all we can do with it.
  - hands on multi-LORA Serving where we have llm and we loaded multiple adapters and
    we are able to actually get the inference of different usecases we are solved by using 1 single docker
    container by using 1 single docker container deployed with multi-LORA serving.
