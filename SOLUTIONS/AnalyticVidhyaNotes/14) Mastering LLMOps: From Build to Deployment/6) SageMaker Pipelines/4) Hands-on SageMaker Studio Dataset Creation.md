Hands-on SageMaker Studio Dataset Creation:
============================================

  - in this lesson , we would be using hugging face to fetch or train our llm model.
  - we would be doing an end to end training on sagemaker studio and for that  we are logged in
    to the hugging face console.
![image](https://github.com/user-attachments/assets/0ae1c5df-5a9f-4ce4-8c04-df721a19b1d6)
![image](https://github.com/user-attachments/assets/6b60bb76-7ee2-4313-892a-f3b97b8d156f)

  - on top of the search bar, we would be selecting the model that we would be training.
  - this is the black forest lab flux1 model.and use this model, you have to **agree to terms and condition**.
![image](https://github.com/user-attachments/assets/90c81ecd-7f74-43f1-a632-d850b8cb2d7b)

  - one we would be using for our lab would be Gemma-2B-IT.
  - accept the terms.
  - once you accept , on top right you can go to settings, > in settings > left you can see > AccessTokens.
![image](https://github.com/user-attachments/assets/52bbe5b2-f1b7-4cef-946e-3d322c6a86e0)

  - we would be generating , also we would want to check the status of the aggrement.
![image](https://github.com/user-attachments/assets/41a9153b-1614-45d8-9575-bc30a8f84806)

  - we also can check the status of the aggrement.
![image](https://github.com/user-attachments/assets/7ceb1f43-eda7-431e-b2ce-5804ad4f8a50)

![image](https://github.com/user-attachments/assets/0c868886-5344-4f26-93d3-6264d945c690)

  - click on read access to repository by access token to read from git repo
  - above click on create token.
  - make sure you copy the token.
  - this would be shown only once.
  - once you have copied this, and your status in git also accepted as above.


SageMaker:
===========

 - login in now to sagemaker and access sagemaker jupyter lab as we did earlier.
 - the notebook we work will be sagemaker training script mode
 - ensure that folder is opened and then we working with gemma notebook.
![image](https://github.com/user-attachments/assets/c22cbb28-c67b-4203-9b08-51b3e36f2e32)
![image](https://github.com/user-attachments/assets/a722c105-e680-48bf-8233-193b24439113)

 - we login into machine which is t3.large , which is very small machine, so the training would be
   happen in sagemaker training job,not in this environment that we have opened up.


Code:
======

step 1: install requirements:
==============================
![image](https://github.com/user-attachments/assets/ef5b06f1-236c-44ad-8a06-91f43fc9a832)



Step 2: add hugging face token:
================================

add env for hugging face api-key

![image](https://github.com/user-attachments/assets/33bf12ae-f12f-407f-ae1a-57fc75078b2a)


![image](https://github.com/user-attachments/assets/1ddd4a88-13ac-4a30-af72-cf8c534e61d5)

  - data set download in local.
  - start instantianting the bucket, sagemaker session
  - once sagemaker session is loaded by sagemaker.session() we would go on to download and clean our dataset.


Step 3: dataset overview:
=========================

![image](https://github.com/user-attachments/assets/39c79c8b-d243-44b8-86aa-48e155268341)
![image](https://github.com/user-attachments/assets/23ff4c0a-476d-43ce-a65a-0de400cdc736)

  - dataset is a library by hugging face.
  - search for this dataset in hugging face, you can see that it is a text to sql based dataset.
  - we are given a lot of background as to what the sql data schema looks like and then once can ask questions
    based on sql schema and for that particular questions answers the sql query will be generated by the model.
![image](https://github.com/user-attachments/assets/4f58f5b8-fbd2-4e44-aa60-2fbba08a5adf)

  - as you can see dataset is now downloaded , see shape, how dataset in train and test.
  - you can do ts.train.features to find out what are the features that is present in the dataset.
  - lets print one of the row.
![image](https://github.com/user-attachments/assets/eb6b6be0-50bc-45a3-86a8-fb99363aa124)

   - so, we can see there is a id. this is the domain of the sql query.
   - so, it is from the forestry domain.
   - comprehensive data on sustainable forest management is one of the column called domain description.
   - now, the sql complexity is also provided.
   - it depends on us, if we want to use it or not , but the complexity of the sql is going to be around single join.


step4 : prepare our dataset:
==============================

    - this is how dataset looks like
    - we are going to get prompt out of it.

![image](https://github.com/user-attachments/assets/55012e19-0b1f-4cf8-a989-53f58d159fa4)

 ![image](https://github.com/user-attachments/assets/caa79b96-b3da-45b7-a3ff-b54ca42231c3)

    - get the tokenizer, we also make sure tokenizer is  also from the same model, because tokenizer has 
      special language which is exactly same as when the model is trained.
    - we load the tokenizer here and then this is mapping function which takes given get message function
      and then provide all the data to this and then now the new formatted values looks something like this.
    - so, we have a value called messages inside which we have content like this , values are inserted 
    - and for the assistant we have final sql query.

![image](https://github.com/user-attachments/assets/b5a9397d-1917-410e-bfc1-c3fbd5cceda7)

    - now, just to see what tokenizer actually adds, is these special tokens which were there during 
      training and how every model has their own way of putting tokens .
    - so, this is <start of turn> end of turn , this is what google  mentions for their particular model it could be different.
    - this could be different for different, but we dont need to take care of remembering this or putting this manually.
    - you can just use the tokenizer of the same model , it will automatically put these values in it.


step5 : upload to S3:
==============================
![image](https://github.com/user-attachments/assets/75a11d26-2813-42fb-b08e-90051acbe7a3)

    - now the data is structured and ready we need to upload to S3.
    - the reason we are uploading to s3 is , because the training job would not be able to use data
      from the local path, that we have it here.
    - we need to specify some s3 path that we can then finally give to the training job from where it will
      download the dataset and after it is downloaded , we also upload the final results and model artificats and somethings like that.
    - so lets upload the dataset to s3 
    - we finally conver our message into json format in every single line.
    - so , finally call the upload file to upload the data.

Step5: amazon dashboard:
=========================

![image](https://github.com/user-attachments/assets/91250c49-5f9a-47d9-80b7-18de27df2119)
![image](https://github.com/user-attachments/assets/f0f358ed-2715-4293-9b33-87374e8662fb)
![image](https://github.com/user-attachments/assets/da9c8774-f7d4-4e8a-a627-90021680e3b3)
![image](https://github.com/user-attachments/assets/9c179e46-6ebc-4d56-8fd1-f3d91902f647)

    - Now, let see and go to s3 and see how our model looks like 
    - we go to the amazon dashboard on the search , we will put s3 
    - click on s3 bucket
    - we gave one of the id, so you can search u1 as well.
    - the bucket name is AVLinMobsageMakerWorkshop.
    - once you click on it you can view the dataset.
    - you can see upload the train.json and test.json
