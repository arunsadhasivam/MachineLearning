
Query Engine and Chat Engine:
==============================



1)Step 1:setup:
================

![image](https://github.com/user-attachments/assets/fba64121-c927-4c73-948c-47037438926f)



2)Step2:download data:
======================
![image](https://github.com/user-attachments/assets/88ab9469-6cc3-4e8f-b71b-a5d4beb9fc64)



3)Step2:load data:
==========================

![image](https://github.com/user-attachments/assets/2aebe4d4-408a-462d-9709-d64c56b25ba7)


<p>
  <details><summary> 1. Query Engine </summary>
    
4)query Engine:
================

- which nodes led to response.
- top 2 source nodes are retrieved.

![image](https://github.com/user-attachments/assets/15144562-0c2f-46dd-8cc5-d2117ab2a5ee)

2nd source node.

![image](https://github.com/user-attachments/assets/0f90edbe-6726-46d7-a22a-bc4dc22978b3)


1st source node : content comes from 1st page of pdf:


![image](https://github.com/user-attachments/assets/f3ab80e3-7cba-44ac-a7d5-1187293b7224)

response

![image](https://github.com/user-attachments/assets/83029267-aec7-4890-83d7-eb99b4653c29)

exact names generated via llm.

by default llamaindex uses openAI-GPT-3.5-turbo model.

![image](https://github.com/user-attachments/assets/4bf6daa1-1399-4741-a374-8996ee7e108b)


Another another question 

![image](https://github.com/user-attachments/assets/1e05892a-d861-469b-b224-6030cc84e648)


- previous result we are only retrieving the top k nodes present in the information.
- now getting complete response.
- Also asking to give only 250 characters possible.

![image](https://github.com/user-attachments/assets/5242d1e9-7630-4fa1-b940-77e2e86cd966)



</details>
</p>



<p>
  <details><summary> 2. Chat Engine </summary>
    
5)Chat Engine:
================

![image](https://github.com/user-attachments/assets/9c1fd32a-814b-4ab9-af41-ec9dd772c2b8)


</details>
</p>
