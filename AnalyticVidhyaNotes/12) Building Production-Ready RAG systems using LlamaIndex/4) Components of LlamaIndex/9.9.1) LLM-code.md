LLM:
===


<p>
<details><summary> 1.LLM Setup OpenAI & Anthropic </summary>

Step 1:environment:
====================

![image](https://github.com/user-attachments/assets/9ad09bcd-071b-4f1c-ba9d-022610df113b)

Step 2-Create Instance:
=======================

create instance of open AI by passing the model .

![image](https://github.com/user-attachments/assets/e791e010-b10d-4177-ab22-a9a9a3c529dd)

</p>
</details>


<p>
<details><summary> 2.1.LLM - Complete Endpoint </summary>

Step 3:call Prompt:
===================

Complete Endpoint:
===================


- First use complete endpoint from the model.
- It takes the zero short instruction or query from the user and directly uses the
  knowledge base to fulfill the response from the model.
- below i want the model to come up with a welcome email to community members of Analytic vidhya.

![image](https://github.com/user-attachments/assets/60724ac2-7b8b-444e-a506-693d3bb6f8e2)
![image](https://github.com/user-attachments/assets/7d705b6d-3103-43cd-a32d-3d6709add264)

you can see above
- how many tokens are used in prompt
- how many tokens are used in response.


dict keys

![image](https://github.com/user-attachments/assets/f788570a-be3f-46a9-a186-a31c8a76a643)


fetch from raw dictionary  key 'usage'

![image](https://github.com/user-attachments/assets/2bee3377-9c23-45a6-aaec-8216df4cf7c2)


</p>
</details>





<p>
<details><summary> 2.2.LLM - Chat Endpoint  </summary>


- chat with the list of messages. chat message api is called
  the advantage of this we can set a "sytem role" and give a broader context to the LLM.
- As a "user" i do some query.

   
![image](https://github.com/user-attachments/assets/34f32bf2-a240-486a-8eb2-f43302e82bdf)
![image](https://github.com/user-attachments/assets/47d35066-cad5-4663-b71f-49b5222831df)
![image](https://github.com/user-attachments/assets/afa3097e-4de2-4b2a-96cb-c9b3d2552ab4)
 
![image](https://github.com/user-attachments/assets/c7a8c577-fae8-4c8e-bb64-703be5a6c80e)
![image](https://github.com/user-attachments/assets/e71f1d23-4892-4475-90e4-b8c1eb5250ca)

</details>  
</p>

<p>
<details><summary> 23.LLM - Stream Chat Endpoint  </summary>

![image](https://github.com/user-attachments/assets/b8cf1451-b02b-4bd8-a59e-21b5658d7bb5)

</details>  
</p>

<p>
<detials><summary>3. Additional Configurations supported</summary>

list of models supported


![link](https://www.platform.openai.com/docs/models)


![image](https://github.com/user-attachments/assets/1f1402d4-de73-4341-babc-3e09b375530b)


![image](https://github.com/user-attachments/assets/5b04473b-3580-403e-a07c-e47adad89bfc)

- default temperature - 0.1 which is less creative
- if you want more creative - set higher value of temperature.

![image](https://github.com/user-attachments/assets/a504fe8d-de27-4180-a319-18def0bfa384)


as you can first just we set 25 stopped generation, second response it says length reached - which is
already length reached.


![image](https://github.com/user-attachments/assets/0e6d7999-7c58-49f1-a562-f1cb50eba8b7)


**Seed-** fix randomness in the response generation. if not fixed the message generated everytime is 
different. it also depends on temperature.
if you want consistent then set temperature to lower value and set the seed value.



 </details> 
</p>




<p>
<details><summary> 4.LLM  Anthropic- Paid </summary>
![image](https://github.com/user-attachments/assets/5cb56717-4243-4215-86a1-79a2bbcdf81c)


<p>
</details>
