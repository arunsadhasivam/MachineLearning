Summary Index:
===============

summary in sequential index. no need of any llm since we dont create embeddings.
default size is 1024.

  ![image](https://github.com/user-attachments/assets/5d907663-d8fa-44e4-b36b-e17d20c60ca5)

Retrieve node no 4

  ![image](https://github.com/user-attachments/assets/1fa89289-71f8-439a-8505-bfcd88595990)


Persist the nodes
As you can see summary index does not need embeddings and no vector store. we dont need llm to create summary index since
no embeddings, so no vector store.

  ![image](https://github.com/user-attachments/assets/a714a00d-87cf-4d26-af61-48a9d31429fc)
  ![image](https://github.com/user-attachments/assets/56aa258a-c573-4aea-8f80-ed0d925cb8e0)


Construct a retriever with llm retriever:
==========================================

    - 2 mdoes (llm mode and embedding mode) entire raw chunk as summary
    - llm mode compare the summary that we have got for each document here.
      basically summary component is nothing but the entire document itself.
      we are not generating any summary . entire raw chunk is considered as summary.
    - llm compares the content present in the node (text content) with the context 
      present in query.
    - that context is compared by llm.it is intrepreted by llm and nodes are retrieved.
    
    

      ![image](https://github.com/user-attachments/assets/145238ff-634c-4820-8b54-6396ed43e345)

    


