{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPRVq3e03c1K"
   },
   "source": [
    "# Conversational UI Chatbot App with ChatGPT, LangChain and Streamlit\n",
    "\n",
    "Here we will build a advanced ChatGPT Conversational UI-based chatbot using LangChain and Streamlit with the following features:\n",
    "\n",
    "- Custom Landing Page\n",
    "- Conversational memory\n",
    "- Result streaming capabilities (Real-time output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1KvMtf54l0d"
   },
   "source": [
    "## Install App and LLM dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.3.11\n",
    "!pip install langchain-openai==0.2.12\n",
    "!pip install langchain-community==0.3.11\n",
    "!pip install streamlit==1.32.2\n",
    "!pip install pyngrok==7.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiwGjVWK4q6F"
   },
   "source": [
    "## Load OpenAI API Credentials\n",
    "\n",
    "Here we load it from a file so we don't explore the credentials on the internet by mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5e1HqI56y7t3"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryheOZuXxa41"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('chatgpt_api_credentials.yml', 'r') as file:\n",
    "    api_creds = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZs7ts6NzADJ",
    "outputId": "26403aa4-c892-46e3-8a24-f7566bbfc094"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['openai_key'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_creds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDe44J0N0NcC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = api_creds['openai_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCMshwB1U9iQ"
   },
   "source": [
    "## Write the app code here and store it in a py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXceMDF0Qza0",
    "outputId": "00f2633f-5de2-4fea-97c8-b31f90bbdfd4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%writefile` not found.\n"
     ]
    }
   ],
   "source": [
    "# the following line is a magic command\n",
    "# that will write all the code below it to the python file app.py\n",
    "# we will then deploy this app.py file on the cloud server where colab is running\n",
    "# if you have your own server you can just write the code in app.py and deploy it directly\n",
    "%%writefile app.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from operator import itemgetter\n",
    "import streamlit as st\n",
    "\n",
    "# Customize initial app landing page\n",
    "st.set_page_config(page_title=\"AI Assistant\", page_icon=\"ðŸ¤–\")\n",
    "st.title(\"Welcome I am AI Assistant ðŸ¤–\")\n",
    "\n",
    "# Manages live updates to a Streamlit app's display by appending new text tokens\n",
    "# to an existing text stream and rendering the updated text in Markdown\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "  def __init__(self, container, initial_text=\"\"):\n",
    "    self.container = container\n",
    "    self.text = initial_text\n",
    "\n",
    "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "    self.text += token\n",
    "    self.container.markdown(self.text)\n",
    "\n",
    "# Load a connection to ChatGPT LLM\n",
    "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.1,\n",
    "                     streaming=True)\n",
    "\n",
    "# Add a basic system prompt for LLM behavior\n",
    "SYS_PROMPT = \"\"\"\n",
    "              Act as a helpful assistant and answer questions to the best of your ability.\n",
    "              Do not make up answers.\n",
    "              \"\"\"\n",
    "\n",
    "# Create a prompt template for langchain to use history to answer user prompts\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\", SYS_PROMPT),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Create a basic llm chain\n",
    "llm_chain = (\n",
    "  prompt\n",
    "  |\n",
    "  chatgpt\n",
    ")\n",
    "\n",
    "# Store conversation history in Streamlit session state\n",
    "streamlit_msg_history = StreamlitChatMessageHistory()\n",
    "\n",
    "# Create a conversation chain\n",
    "conversation_chain = RunnableWithMessageHistory(\n",
    "  llm_chain,\n",
    "  lambda session_id: streamlit_msg_history,  # Accesses memory\n",
    "  input_messages_key=\"input\",\n",
    "  history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# Shows the first message when app starts\n",
    "if len(streamlit_msg_history.messages) == 0:\n",
    "  streamlit_msg_history.add_ai_message(\"How can I help you?\")\n",
    "\n",
    "# Render current messages from StreamlitChatMessageHistory\n",
    "for msg in streamlit_msg_history.messages:\n",
    "  st.chat_message(msg.type).write(msg.content)\n",
    "\n",
    "# If user inputs a new prompt, display it and show the response\n",
    "if user_prompt := st.chat_input():\n",
    "  st.chat_message(\"human\").write(user_prompt)\n",
    "  # This is where response from the LLM is shown\n",
    "  with st.chat_message(\"ai\"):\n",
    "    # Initializing an empty data stream\n",
    "    stream_handler = StreamHandler(st.empty())\n",
    "    config = {\"configurable\": {\"session_id\": \"any\"},\n",
    "              \"callbacks\": [stream_handler]}\n",
    "    # Get llm response\n",
    "    response = conversation_chain.invoke({\"input\": user_prompt},\n",
    "                                         config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8de1tM6FVLsq"
   },
   "source": [
    "## Start the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py --server.port=8989 &>./logs.txt &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above command doesn't work, use the below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Za_TAI2RkPI9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8989\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.1.36:8989\u001b[0m\n",
      "\u001b[0m\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py --server.port=8989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrVhyQVirAqP",
    "outputId": "bafc38ab-f87c-4d72-c64c-a2809cdb60da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit App: https://0430-35-221-238-153.ngrok-free.app\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "import yaml\n",
    "\n",
    "# Terminate open tunnels if exist\n",
    "ngrok.kill()\n",
    "\n",
    "# Setting the authtoken\n",
    "# Get your authtoken from `ngrok_credentials.yml` file\n",
    "with open('ngrok_credentials.yml', 'r') as file:\n",
    "    NGROK_AUTH_TOKEN = yaml.safe_load(file)\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN[ngrok_key])\n",
    "\n",
    "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
    "ngrok_tunnel = ngrok.connect(8989)\n",
    "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q3yFB_jsgC5"
   },
   "source": [
    "## Remove running app processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pG7Abg_LrAw6"
   },
   "outputs": [],
   "source": [
    "ngrok.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2VA4ZtCkPNN",
    "outputId": "f3d7ca23-ff25-4381-c576-63ca7a3d6362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root         637       1  1 03:14 ?        00:00:06 /usr/bin/python3 /usr/local/bin/streamlit run ap\n",
      "root        2854     203  0 03:23 ?        00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
      "root        2856    2854  0 03:23 ?        00:00:00 grep streamlit\n"
     ]
    }
   ],
   "source": [
    "!ps -ef | grep streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WxGAxGHkPLP"
   },
   "outputs": [],
   "source": [
    "!sudo kill -9 637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlBQh5fdkPPY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
