Hands-on Model Evaluation:
==========================


   - once the training step is done , we are assuming that the final trained model artificats then be saved in s3.
   - since we gave the model output path.
   - what we do now is evaluation.


Evaluation Step:
=================
![image](https://github.com/user-attachments/assets/0045fc67-495b-4164-b625-f19c3f380080)

    - to evaluate the model we need 2 things.
      1) evaluation datasets which was created from preprocessing path.
      2) model artificats to create a model.

    - also for evaluation , we dont specifically need GPU based machine , we can do it in CPU in many cases.
    - that is one of the advantages of sagemaker as well.
    - we create an another step known as evaluation step. over this we do only CPU based output evaluation and
      save those evaluation to be used in further pipeline.

    - to see it, this is what evaluation step is :
![image](https://github.com/user-attachments/assets/2cb58fa3-11b0-4b56-a6a9-f96d771bbb59)

    1) after training , it has to load the model, 
    2) create the test dataset
    3) do the prediction against test dataset.
    4) finally create this metric, that we can save in one of the evaluation report file.
    5) we can also see the accuracy in sagemaker ui.


   - we then save this evaluation  report to the evaluation directory.
   - we can then use this evaluation on one of the future step as well.
   - to do all of this , we are going to use script mode now.

![image](https://github.com/user-attachments/assets/faf3db36-c1ce-4bf0-b1e7-84199b2b6d1f)

   - code is present in evaluation.py.
   - in code , the model is expected to be available at this path /opt/m1/processing/model/model.tar.gz.
   - read the dataset and finally do the model.predict() on the dataset.
   - we get the prediction using the actual output and predicted output - we calculate MSE(mean square Error)
   - if the error is too high , we dont want to actually deploy the model and we want purposefully fail the pipeline.
   - so, we can create the report dictionary ,where we take the mean square error calculated and standard deviation
     and save it in this particular path(/opt/m1/processing/evaluation/evaluation.json)
   - we would then in later stage use this file, to figure out how the further steps would happen.
      1)one thing is if MSE if high - fail the pipeline
      2) else this model be deployed to  the pipeline.
   - or atleast we created a model, but wait as our status has **pending_for_approval**.
   - and then when the approval_status is created as done , this model can automatically be deployed as model endpoint.
   - once these evaluation.py file is ready, we can specify this in the evaluation step.
   - now we create a evaluation step.
 ![image](https://github.com/user-attachments/assets/23158a29-6642-4340-bfd6-b720637283a7)


Evaluation Step:
===================
  - we will use the script processor, with the given image_uri and then the command will be python,
  - then we mention the instance type, base job name, role
  - session_type mentioned is pipeline and then we would run the **script_eval** function
![image](https://github.com/user-attachments/assets/5ace7dfa-2a0c-4c59-ba4b-d6118253315c)

  - this is image_url which we mentioned before 
  - xgboost will be available in the same docker container.
  - and the script file while doing an import xgboost ,numpy will still works.
  - xgboost,numpy are other libraries which are available to run.
  - script_eval.run() will not obviously not run , because it is a pipeline that is specified.
  - input - is a processing input
  - now , we are not train taking the input from training step, we already know the s3 path would be , but
    we have to call the processing step to get the path of the test set and we have to call the **model training
    step** to get the **model artificat path**.
  - so, this is how it is, we can call **step_train.properties.ModelArtificats.s3ModelArtifacts** to get the artifacts
  - **step_train** is one of the step that we have created over here.
  - in a similar way we have **step_process** which we created in the first place, where we simply get the test channel
    and get its **s3_uri** and provided at the **destination_folder**(/opt/m1/processing/test) like this .

  - the output which is the json file that we created(/opt/m1/processing/evaluation/evaluation.json) has mentioned as
    processing output , output name as evaluation and the source would be present in the folder which would be copied
    back to s3.
  - the code is available in code/evaluation.py file and these are the arguments that we need to create the evaluation steps.

  - we would run this.
  - now the evaluation report can be created as one of the file called as property file , just mentioned it as
    name, path as evaluation.json which can then be used in the further steps.

![image](https://github.com/user-attachments/assets/95270568-810a-4a79-aa85-2acdea31a565)

  - we finally created the evaluation steps as a processing steps, because the type of step could either be
    training,processing,transform and all of that. so there is no step like evaluation step, evaluation is
    just data processing, so we can run evaluation in one of the step which is type processing.

  - so , we create a processing_step , we name it as avalon_step ,we give it the step_arguments and we also
    mention the property file as the evaluation report_file that gets created.

