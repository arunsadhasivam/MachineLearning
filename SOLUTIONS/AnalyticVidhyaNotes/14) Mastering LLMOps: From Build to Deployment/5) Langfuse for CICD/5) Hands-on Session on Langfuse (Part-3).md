![image](https://github.com/user-attachments/assets/8789ed22-b947-4402-8c73-95f599dd9912)Hands-on Session on Langfuse (Part-3):
======================================


  - Another thing that is happenning a lot , is after the output is generated by the model.
  - sometimes you want to run additional evaluation, for that you can create a custom template.
  - you can use that custom template on these evaluations in traces.
  - and then generate certain kind of output.
  - to know in details what i mean is when the phrase is complete, and the output is generated
     if for this paticular trace, we want another openAI or some other llm based evaluation output.
  - we can simply run the trace input and output and can get the evaluation needed.
  - so , for this trace the input was this, and the output genrated was this, we can actually
    create evaluations.
  - for evaluations, we go to templates, and let us try to detect what was the language which the user
     give us a input and the output that the llm gave back.
  - we want to make sure all of the calls that are going to make to the model actually consist of 
    same languages as input and output.
  - and you want to somehow track if this was not the case in some of the generations and to do that
    we have evaluations.
  - so, we click on > add evaluation template > so we will create a template and we can give the template name - **languagecheck**
  - as you can see the prompt is evaluate the query different text languages and the generation on a continuous scale from 0 to 1.
  - the generation can be consider as correct which is full score. if the generation language is same as the query language.
  - now, we are going to do few shorts examples.
  - the input query is in italian but the generation is actually in english.
  - so , the scope for this actually zero.
  - give a reasoning.
  - give the temperature 0.
  - Top P values , this is going to use one of the model that we created and click on save.
![image](https://github.com/user-attachments/assets/06f80392-ded7-414e-8f36-e08e5c3b6999)


    - create a evaluation config 
![image](https://github.com/user-attachments/assets/fa52903c-69f1-482c-a19f-c0e4b7d80bd3)
    - this filter will decide , for which trace the evaluations should run.
    - so , we would filter by tags. and if any of the tags consists of langcheck and we want to run .
    - these tags are automatically created based on other traces or other tags are generated.
![image](https://github.com/user-attachments/assets/97511788-a7f0-4465-84a6-53a677529a7e)

    - so , in one of the previous trace  , we create a tag there could be multiple tag that we
      created at different places. we could also manually created.
    - but then all the list of tags will automatically present over here and then we select over
      the multiple tag which one we want.
    - so, we would always end up adding a tag called langcheck and if the tag happens run this evaluation.
    - so this is the filter criteria,
        when tag is any of ---> or all --> this particular given tag.
![image](https://github.com/user-attachments/assets/82f60ee8-0687-4511-a3ff-95f0bc8e9d90)

    - Now , we want to make sure that the trace input and output is entered in to evaluation template
      and so it is going to ask if the query and the generation variable should be substituted with what variable.
     - so , the query should be substituted by the trace input which is what user entered and the generation to
       be substituted by the trace output.
     - you can also go deeper and create the span generation and event-based input and output that we see in the
       full-fledged sequence tree.
     - but for now we will keep it simple.
     - and then the trace input and output is what we want to substitute in the query and generation.
     - samplling ensures that not every time these evaluations run , but 50% would be that out of 100 calls
       having the tag as **langcheck** , half of them would be only be running through the evaluation.
     - we will setting it to 100 to make sure the evaluation will run for sure.
     - and then we can also set a delay value, this delay value is after how many seconds to run this evaluations
     - just to ensure that all the output is present
     - so , we will just set this value as 5.
     - as you can see this evaluation is now active
     - now, we will go ahead and just use the same call that we did before that the difference now would be
       that we have the tag   as **langcheck** and the input is italian.
     - if we run this now, this gives us an output , which if we goes to traces we could see over here.
![image](https://github.com/user-attachments/assets/c479e731-3b5e-4647-9e91-851bbab18b36)

      - Now, diffrence is the score is generated from 0 and 1 as we mentioned and the score is **1 which means
        the language matches the input and output**.
![image](https://github.com/user-attachments/assets/f4ba9bbd-0620-4d4c-b1f3-29fdb3681e0f)
![image](https://github.com/user-attachments/assets/132ee4cc-203f-4972-b840-6a919863bff2)

     - We can see that the tag was **langcheck** and if i click on it just like before i can see all the output generated in depth. 
     - and if i click on scores, i can see that this particular evaluation was run at this particular time , the name was
       **langCheckV1** , the datatype was numeric and comment is one of the values that we give , which was
       **"give a one word output of the query language detected"**

click on templates
![image](https://github.com/user-attachments/assets/58833157-9b26-404e-8dfe-c85e7fdc3eff)
![image](https://github.com/user-attachments/assets/e7f974c9-130a-4c28-b5e7-366d34be8daa)
![image](https://github.com/user-attachments/assets/6004c686-6811-4775-8033-68413ad8f7cc)

     - dashboard is exactly in templates here and you can see that we told the evaluation config specifically
       to also tell us the reason and it was addition data for generated in our score.

![image](https://github.com/user-attachments/assets/9a621dd7-a71d-4b5a-94de-ef87b12bcd65)
![image](https://github.com/user-attachments/assets/18f2032f-b206-438b-852b-ca30a96a1bb4)

click on score tab

![image](https://github.com/user-attachments/assets/d53a64bb-8f3f-47c8-8d9e-29f7ccf5adf1)


- so , in this way we can run lot of these evaluations and then these evaluations can be used in
  dashboard to see and track different metrics going up and down.
- these are certain measures , we want to make sure that are in place and we can always go into
  the dashboard and track these values dont go up over time.
- and are multiple other checks that we can do .you also have certain pre-build templates.
![image](https://github.com/user-attachments/assets/dee4f954-536b-4d27-b61a-168ed733950e)

- so, we created our own template over here , but if you click on add evaluation template
  we have lot of hallucinations toxicity helpfulness based templates that we can use by default in the
  exact same method may be add different tag to it and then we can run that specifically for evaluation.
