{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1Y3gSVdHdzb"
   },
   "source": [
    "# Router Query Engine\n",
    "\n",
    "Routers serve as specialized modules designed to process a user's query and select from a set of predefined \"choices,\" characterized by their metadata.\n",
    "\n",
    "There are two primary types of core router modules:\n",
    "\n",
    "1. **LLM Selectors:** These selectors present the available choices as a text prompt, utilizing the LLM text completion endpoint for decision-making.\n",
    "\n",
    "2. **Pydantic Selectors:** Here, choices are passed in the form of Pydantic schemas to a function-calling endpoint. The results are then returned as Pydantic objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV4KnOmXUPC-"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Install `llama-index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13649,
     "status": "ok",
     "timestamp": 1698333976502,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "uOZ6iXTJHlQ5",
    "outputId": "c06ede1c-43ef-4b6b-916e-f46fb81b43c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.8.51.post1-py3-none-any.whl (792 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.6/792.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.22)\n",
      "Collecting aiostream<0.6.0,>=0.5.2 (from llama-index)\n",
      "  Downloading aiostream-0.5.2-py3-none-any.whl (39 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from llama-index)\n",
      "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
      "Collecting langchain>=0.0.303 (from llama-index)\n",
      "  Downloading langchain-0.0.323-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
      "Collecting openai>=0.26.4 (from llama-index)\n",
      "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index)\n",
      "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.5.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting urllib3<2 (from llama-index)\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->llama-index)\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (4.0.3)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain>=0.0.303->llama-index)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.43 (from langchain>=0.0.303->llama-index)\n",
      "  Downloading langsmith-0.0.52-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (1.10.13)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (2.31.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.1.3)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->llama-index)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->llama-index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.303->llama-index) (2023.7.22)\n",
      "Installing collected packages: urllib3, mypy-extensions, marshmallow, jsonpointer, deprecated, aiostream, typing-inspect, jsonpatch, tiktoken, openai, langsmith, dataclasses-json, langchain, llama-index\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiostream-0.5.2 dataclasses-json-0.5.14 deprecated-1.2.14 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.323 langsmith-0.0.52 llama-index-0.8.51.post1 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.28.1 tiktoken-0.5.1 typing-inspect-0.9.0 urllib3-1.26.18\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_Ttz5WDI8M6"
   },
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9454,
     "status": "ok",
     "timestamp": 1698334064081,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "LGEOh3sdFuQ-",
    "outputId": "7be5e69a-c15c-4c5c-ae87-2b5a230fff0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumExpr defaulting to 2 threads.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Set up the root logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)  # Set logger level to INFO\n",
    "\n",
    "# Clear out any existing handlers\n",
    "logger.handlers = []\n",
    "\n",
    "# Set up the StreamHandler to output to sys.stdout (Colab's output)\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)  # Set handler level to INFO\n",
    "\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(handler)\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SummaryIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    ")\n",
    "\n",
    "import openai\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# Setup openai api key\n",
    "openai.api_key = 'YOUR OPENAI API KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_J2AEezZP4Uf"
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1698334067111,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "4wlP1wKgP3yV",
    "outputId": "74260a4a-0d9b-41bd-e19d-cb392b220745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-26 15:27:46--  https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
      "\n",
      "\r",
      "          data/paul   0%[                    ]       0  --.-KB/s               \r",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2023-10-26 15:27:46 (5.32 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f1Awi4MVZRf"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3248,
     "status": "ok",
     "timestamp": 1698334094489,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "TozkIS3mF8DC",
    "outputId": "27a9d1b7-47db-45d7-a67c-09101a6577cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"data/paul_graham\").load_data()\n",
    "\n",
    "# initialize service context (set chunk size)\n",
    "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4LNrmhrK4xe"
   },
   "source": [
    "## Define Summary Index and Vector Index over Same Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9C2Gm1UF8r3"
   },
   "outputs": [],
   "source": [
    "# Summary Index for summarization questions\n",
    "summary_index = SummaryIndex(nodes)\n",
    "\n",
    "# Vector Index for answering specific context questions\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voktqsQNLApj"
   },
   "source": [
    "## Define Query Engines.\n",
    "\n",
    "1. Summary Index Query Engine.\n",
    "2. Vector Index Query Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLMKbMAUGA9f"
   },
   "outputs": [],
   "source": [
    "# Summary Index Query Engine\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    "    service_context = service_context\n",
    ")\n",
    "\n",
    "# Vector Index Query Engine\n",
    "vector_query_engine = vector_index.as_query_engine(service_context = service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzm7L0MzYQUR"
   },
   "source": [
    "## Build summary index and vector index tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fg4aOwPaGNxb"
   },
   "outputs": [],
   "source": [
    "from llama_index.tools.query_engine import QueryEngineTool\n",
    "\n",
    "# Summary Index tool\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=\"Useful for summarization questions related to Paul Graham eassy on What I Worked On.\",\n",
    ")\n",
    "\n",
    "# Vector Index tool\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=\"Useful for retrieving specific context from Paul Graham essay on What I Worked On.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0zHAQS_LF3r"
   },
   "source": [
    "## Define Router Query Engine\n",
    "\n",
    "Various selectors are at your disposal, each offering unique characteristics.\n",
    "\n",
    "Pydantic selectors, supported exclusively by gpt-4-0613 and the default gpt-3.5-turbo-0613, utilize the OpenAI Function Call API. Instead of interpreting raw JSON, they yield pydantic selection objects.\n",
    "\n",
    "On the other hand, LLM selectors employ the LLM to generate a JSON output, which is then parsed to query the relevant indexes.\n",
    "\n",
    "For both selector types, you can opt to route to either a single index or multiple indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTncjEj2LH88"
   },
   "source": [
    "## PydanticSingleSelector\n",
    "\n",
    "Use the OpenAI Function API to generate/parse pydantic objects under the hood for the router selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGjl2y5QGRcW"
   },
   "outputs": [],
   "source": [
    "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.selectors.llm_selectors import LLMSingleSelector, LLMMultiSelector\n",
    "from llama_index.selectors.pydantic_selectors import (\n",
    "    PydanticMultiSelector,\n",
    "    PydanticSingleSelector,\n",
    ")\n",
    "\n",
    "# Create Router Query Engine\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=PydanticSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11130,
     "status": "ok",
     "timestamp": 1698334234628,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "0hywia-DGTx2",
    "outputId": "3810695c-327b-4b3c-fea9-def722100d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting query engine 0: This choice is specifically mentioned as useful for summarization questions..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1663 request_id=05f4bc40c48d57cee65224592f507ffb response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3454 request_id=1ab0df88737640a055cf07a645327e0f response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3200 request_id=e71b7608c1ab09d064c07948f3b2cb3b response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3640 request_id=196158cac541e87a52869bf64436d4b2 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3501 request_id=0328626e5dfa525a1ca0270556b87427 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4414 request_id=9aaf251b36845f38c326814257b765f1 response_code=200\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1698334241283,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "sriu-0zoLR7q",
    "outputId": "88cab857-1e95-4eaf-d119-cee06013892d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:20px\">The document provides a narrative of the author's experiences and interests in writing, programming, art, and entrepreneurship. It starts with their early experiences in writing and programming, their fascination with artificial intelligence, and their transition to microcomputers. The author then discusses their interest in Lisp programming and their decision to pursue art. They describe their experiences as an art student, their time at a software company, and their decision to start their own company, which builds online stores. The document also discusses the challenges and successes of building the company, the growth rate of startups, and the decision to leave the company. The author reflects on the power of online publishing, their involvement in various projects, and their personal experiences that led to their decision to leave their role at Y Combinator. The document concludes with the author reflecting on their past choices and considering future projects.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zkEOzNYLUQ_"
   },
   "source": [
    "## LLMSingleSelector\n",
    "\n",
    "Utilize OpenAI (or another LLM) to internally interpret the generated JSON and determine a sub-index for routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6i5dK4Muuaz"
   },
   "outputs": [],
   "source": [
    "# Create Router Query Engine\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10361,
     "status": "ok",
     "timestamp": 1698336346707,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "_y-1ztZ6utGp",
    "outputId": "5bc8a8d9-8a6d-408e-a63a-ae1bbc445b97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting query engine 0: The summary of the document is related to Paul Graham's essay on What I Worked On..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1750 request_id=200c1a6ac1dd410b2949e20280a3ffa7 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2227 request_id=f38a68434e62ee7bf5fce7ef0a6abda4 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3185 request_id=ce2c3240d255a15883201d5f24b666d0 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3271 request_id=927af0471aaeab67bfec171162b1e858 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3644 request_id=271b1a920ceeae812fb5e17b14a8d5b6 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4237 request_id=fd27aef736a5366700c5f77fcedf2445 response_code=200\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1698336350750,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "vwgHKW-qusYR",
    "outputId": "130038dc-a965-4dc2-e793-7e0487fedcb1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:20px\">The document provides a personal account of the author's experiences and interests in writing, programming, and art. It discusses their early experiences in writing and programming, their fascination with artificial intelligence, and their decision to focus on Lisp programming. The document also covers their experiences studying art, their transition into the tech industry, and the creation and development of their software company. It touches on their involvement in various projects, including publishing essays online and starting an investment firm. The document concludes with the author's contemplation of future projects and their return to writing essays.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4036,
     "status": "ok",
     "timestamp": 1698336369564,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "rMgPIzfvuriI",
    "outputId": "22ea75dc-dc00-4482-af00-d972f67deeb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting query engine 1: The question is asking for specific context about what Paul Graham did after RICS, which is better suited for retrieving specific context from the essay..\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did Paul Graham do after RICS?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1698336376616,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "Bzo6X5W2uqpe",
    "outputId": "c1e75d00-d12d-4441-e046-1a329aea2d23"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:20px\">After RICS, Paul Graham applied to art schools and was accepted into the BFA program at RISD. He then went on to attend art classes at Harvard and eventually pursued a career as an artist.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MqCihafLZyE"
   },
   "source": [
    "## PydanticMultiSelector\n",
    "\n",
    "If you anticipate queries being directed to multiple indexes, it's advisable to use a multi-selector. This selector dispatches the query to various sub-indexes and subsequently aggregates the responses through a summary index to deliver a comprehensive answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEV_L_mkWL7V"
   },
   "source": [
    "## Let's create a simplekeywordtable index and corresponding tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hexv6i0runj8"
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleKeywordTableIndex\n",
    "\n",
    "keyword_index = SimpleKeywordTableIndex(nodes)\n",
    "\n",
    "keyword_query_engine = keyword_index.as_query_engine(service_context=service_context)\n",
    "\n",
    "keyword_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=keyword_query_engine,\n",
    "    description=\"Useful for retrieving specific context using keywords from Paul Graham essay on What I Worked On.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBLyUgEYWTcc"
   },
   "source": [
    "## Build a router query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "as9REJo7ulzu"
   },
   "outputs": [],
   "source": [
    "query_engine = RouterQueryEngine(\n",
    "    selector=PydanticMultiSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        vector_tool,\n",
    "        keyword_tool,\n",
    "        summary_tool\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDIFO7v9uixW"
   },
   "outputs": [],
   "source": [
    "# This query could use either a keyword or vector query engine, so it will combine responses from both\n",
    "response = query_engine.query(\n",
    "    \"What were noteable events and people from the authors time at Interleaf and YC?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuI1xYLN-rKe"
   },
   "outputs": [],
   "source": [
    "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1T71Oegv2f9uR7pFkOWcFX9NxL6S3d2En",
     "timestamp": 1702791935731
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
