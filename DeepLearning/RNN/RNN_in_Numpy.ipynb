{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Simple Recurrent Neural Network (RNN) for Regression\n",
        "\n",
        "* This notebook demonstrates the implementation of a basic Recurrent Neural Network (RNN) using Python and NumPy for regression tasks. The RNN model is designed to predict outputs based on input sequences and corresponding labels. While this example focuses on regression, the configuration can be easily adapted for classification tasks and other purposes."
      ],
      "metadata": {
        "id": "BVJH2SZBrwIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Op8qps02rzKC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Generation\n",
        "\n",
        "- `sequence_length = 10`: Sets the length of input sequences.\n",
        "- `X_train = np.random.rand(100, sequence_length)`: Generates 100 random input sequences of length `sequence_length`.\n",
        "- `y_train = np.sum(X_train, axis=1).reshape(-1, 1)`: Generates labels by summing each input sequence and reshaping to a column vector.\n"
      ],
      "metadata": {
        "id": "v36F4Efmpj0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sequence length\n",
        "sequence_length = 10\n",
        "\n",
        "# Generate random input sequences\n",
        "X_train = np.random.rand(100, sequence_length)\n",
        "\n",
        "# Generate labels (example: sum of the input sequence)\n",
        "y_train = np.sum(X_train, axis=1).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "mjBJTDSoobLV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Initialization\n",
        "\n",
        "- `input_size`, `hidden_size`, `output_size`: Define the sizes of input, hidden, and output layers.\n",
        "- Weight and bias initialization:\n",
        "  - `Wxh`, `Whh`, `Why`: Weight matrices from input to hidden, hidden to hidden, and hidden to output layers respectively.\n",
        "  - `bh`, `by`: Bias vectors for the hidden and output layers.\n"
      ],
      "metadata": {
        "id": "AHs22lWJpnaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = sequence_length\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "# Initialize weights and biases\n",
        "Wxh = np.random.randn(input_size, hidden_size) * 0.01  # Input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
        "Why = np.random.randn(hidden_size, output_size) * 0.01  # Hidden to output\n",
        "bh = np.zeros((1, hidden_size))  # Hidden bias\n",
        "by = np.zeros((1, output_size))  # Output bias\n",
        "print(Wxh.shape,Whh.shape,Why.shape,bh.shape,by.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hObI5zpwp7Dl",
        "outputId": "a34afaa1-847e-4175-a578-47480dbe3015"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 64) (64, 64) (64, 1) (1, 64) (1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Activation Functions\n",
        "* tanh(x): Defines the hyperbolic tangent activation function.\n",
        "* dtanh(x): Computes the derivative of the hyperbolic tangent function."
      ],
      "metadata": {
        "id": "eaSdkIgqpxIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function (tanh)\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "# Derivative of tanh\n",
        "def dtanh(x):\n",
        "    return 1 - np.square(np.tanh(x))\n"
      ],
      "metadata": {
        "id": "KmSqlnZsqM-Y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Forward Pass Function (forward_pass)\n",
        "* Inputs: inputs (input sequences), targets (labels), h_prev (previous hidden state).\n",
        "* Variables Initialization: xs, hs, ys, ps dictionaries to store input vectors, hidden states, pre-activation outputs, and final predictions.\n",
        "* Forward Pass Loop: Iterates through each time step of the input sequences to compute hidden states and predictions.\n",
        "* Loss Calculation: Computes the mean squared error loss between predictions (ps) and targets (targets).\n",
        "* Returns: Loss, predictions (ps), final hidden state (hs), and input vectors (xs)."
      ],
      "metadata": {
        "id": "UewHMnQ7qRR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(inputs, targets, h_prev):\n",
        "    # Initialize variables to store inputs, hidden states, and outputs\n",
        "    xs, hs, ys, ps = {}, {}, {}, {}\n",
        "    hs[-1] = np.copy(h_prev)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # Forward pass\n",
        "    for t in range(len(inputs)):\n",
        "        xs[t] = inputs[t].reshape(1, -1)\n",
        "        hs[t] = tanh(np.dot(xs[t], Wxh) + np.dot(hs[t-1], Whh) + bh)\n",
        "        ys[t] = np.dot(hs[t], Why) + by\n",
        "        ps[t] = ys[t]  # Output without activation for simplicity\n",
        "\n",
        "        # Compute loss (example: mean squared error)\n",
        "        loss += np.mean(np.square(ps[t] - targets[t]))\n",
        "\n",
        "    return loss / len(inputs), ps, hs, xs  # Return xs along with other outputs\n"
      ],
      "metadata": {
        "id": "oYhNRULwqY-D"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop Details\n",
        "\n",
        "1. **Initialization**:\n",
        "   - `learning_rate = 0.01`: Sets the learning rate for gradient descent optimization.\n",
        "   - `num_epochs = 1000`: Defines the number of training epochs.\n",
        "\n",
        "2. **Training Loop**:\n",
        "   - Iterates through each epoch (`for epoch in range(num_epochs)`) to train the model.\n",
        "   - Initializes the hidden state (`h_prev`) before each epoch.\n",
        "\n",
        "3. **Forward Pass**:\n",
        "   - Calls the `forward_pass` function to compute loss, predictions (`ps`), hidden states (`hs`), and input vectors (`xs`).\n",
        "\n",
        "4. **Backpropagation Through Time (BPTT)**:\n",
        "   - Computes gradients (`dWxh`, `dWhh`, `dWhy`, `dbh`, `dby`) for weight matrices and bias vectors using backpropagation through time (BPTT).\n",
        "   - Accumulates gradients across time steps (`for t in reversed(range(sequence_length))`) and updates the gradients for each weight and bias.\n",
        "\n",
        "5. **Weight and Bias Updates**:\n",
        "   - Updates weights and biases (`Wxh`, `Whh`, `Why`, `bh`, `by`) using the computed gradients and learning rate.\n",
        "\n",
        "6. **Monitoring Progress**:\n",
        "   - Prints the loss every 100 epochs to monitor training progress (`if epoch % 100 == 0`).\n",
        "\n",
        "This training loop implements the training process for a recurrent neural network (RNN) using backpropagation through time (BPTT) and stochastic gradient descent (SGD) optimization. It iteratively updates the model's weights and biases to minimize the loss over the training data.\n"
      ],
      "metadata": {
        "id": "keADhdrvqw8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "num_epochs = 100\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Initialize hidden state\n",
        "    h_prev = np.zeros((1, hidden_size))\n",
        "\n",
        "    # Forward pass\n",
        "    loss, ps, hs, xs = forward_pass(X_train, y_train, h_prev)  # Retrieve xs\n",
        "\n",
        "    # Backpropagation through time (BPTT)\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "    dh_next = np.zeros_like(h_prev)\n",
        "\n",
        "    # Backward pass through time\n",
        "    for t in reversed(range(sequence_length)):\n",
        "        # Compute gradients for output layer\n",
        "        dy = 2 * (ps[t] - y_train[t])  # Example: derivative of MSE loss\n",
        "        dWhy += np.dot(hs[t].T, dy)\n",
        "        dby += dy\n",
        "\n",
        "        # Backpropagate gradients to previous time step\n",
        "        dh = np.dot(dy, Why.T) + dh_next\n",
        "        dhraw = dtanh(hs[t]) * dh\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(xs[t].T, dhraw)\n",
        "        dWhh += np.dot(hs[t-1].T, dhraw)\n",
        "        dh_next = np.dot(dhraw, Whh.T)\n",
        "\n",
        "    # Update weights and biases using gradients and learning rate\n",
        "    Wxh -= learning_rate * dWxh\n",
        "    Whh -= learning_rate * dWhh\n",
        "    Why -= learning_rate * dWhy\n",
        "    bh -= learning_rate * dbh\n",
        "    by -= learning_rate * dby\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0Nq6rtKpD96",
        "outputId": "46d6211f-19d4-46f7-97b2-70ed8c8121c9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 25.234006131569632\n",
            "Epoch 10, Loss: 0.5730656976535442\n",
            "Epoch 20, Loss: 0.5423560123253064\n",
            "Epoch 30, Loss: 0.5102393562194583\n",
            "Epoch 40, Loss: 0.47868200127174476\n",
            "Epoch 50, Loss: 0.45161525097558886\n",
            "Epoch 60, Loss: 0.4305648025442177\n",
            "Epoch 70, Loss: 0.414982118508304\n",
            "Epoch 80, Loss: 0.40356544889374446\n",
            "Epoch 90, Loss: 0.3952657903344646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Conclusion\n",
        "* This RecurrentNeural network architecture consists of an input layer, a hidden layer with tanh activation, and an output layer for regression. It uses backpropagation through time (BPTT) with mean squared error loss for training.\n",
        "\n",
        "* In the previous example, we utilized RNNs for regression tasks. There are numerous real-life scenarios where RNNs are applied for regression purposes.\n",
        "* for example :\n",
        "\n",
        "  1. **Energy Consumption Prediction:**\n",
        "    - **Scenario**: Use historical energy usage data and weather conditions (temperature, humidity) to predict future energy consumption.\n",
        "\n",
        "  2. **Sales Forecasting for Retail Stores:**\n",
        "    - **Scenario**: Utilize historical sales data, promotional activities, and seasonal factors to forecast future sales in retail stores.\n",
        "* Feel free to experiment with different architectures, activation functions, and hyperparameters to further explore neural network training.\n"
      ],
      "metadata": {
        "id": "dnYHYlbYr27v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yMjrgLKW1bCR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}