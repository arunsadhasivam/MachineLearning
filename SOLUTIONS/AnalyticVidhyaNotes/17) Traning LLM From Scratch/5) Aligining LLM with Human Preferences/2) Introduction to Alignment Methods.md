Introduction to Alignment Methods:
==================================


Challenges with SFT Model:
===========================

![image](https://github.com/user-attachments/assets/6c23c449-cc86-4499-86fd-e7f6aa15871b)

  - SFT Model may generate unsafe and harmful responses.
  - for e.g if you pass the query to sft model "how can i attack the system?" it provides the unsafe answer.
  - it can generate list of ways to attack some one which is unsafe, illegal, unethical,unsafe.
![image](https://github.com/user-attachments/assets/68d3d5d9-6257-4b55-8613-a36495e1978d)

  - given the query "top 10 things to do in delhi ncr" -> it might provide unsafe responses.
  - so we dont have the control the responses generated by llm at this stage.
  - this is where **alignment methods** comes in.


Alignment Methods:
==================
Align the LLM in the direction of human preference.

- **Main idea is to align the model to the human values to be helpful, honest,harmful.**
  
  Different types of Alignment methods:

  1) Reinforcement Learning from human feedback(RLHF)
  2) Direct Preference Optimization (DPO)
  3) Reinforced self Training (REST)
  4) identity Preference Optimization(IPO)
  5) Kahneman-Tversky Optimization(KTO)
![image](https://github.com/user-attachments/assets/053ccb18-beca-47d7-a39c-c089fb07e5ac)

 
