{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sat_sum</th>\n",
       "      <th>hs_gpa</th>\n",
       "      <th>fy_gpa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>488</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>464</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>380</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>428</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sat_sum  hs_gpa  fy_gpa\n",
       "0      508    3.40    3.18\n",
       "1      488    4.00    3.33\n",
       "2      464    3.75    3.25\n",
       "3      380    3.75    2.42\n",
       "4      428    4.00    2.63"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "data = pd.read_csv('Prodigy University Dataset.csv')\n",
    "# Split the data into features (X) and target (y)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Converting data to numpy\n",
    "X = data[['sat_sum', 'hs_gpa']].values\n",
    "# reshape the fy_gpa into a 2D array with [data_size] rows and 1 column\n",
    "y = data['fy_gpa'].values.reshape(-1, 1)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the features so that it is easier to train the data\n",
    "scaler = StandardScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test= scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Convert numpy to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with 2 neurons\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation\n",
    "preds = model(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2094],\n",
       "        [-0.1946],\n",
       "        [-0.0843],\n",
       "        [-0.1771],\n",
       "        [-0.1015]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.3339, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculating Loss\n",
    "criterion = MSELoss()\n",
    "loss = criterion(preds, y_train_tensor)\n",
    "print(loss)\n",
    "# to learners: You may get different values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing predictions on X_train with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2094],\n",
       "        [-0.1946],\n",
       "        [-0.0843],\n",
       "        [-0.1771],\n",
       "        [-0.1015]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000],\n",
       "        [3.1100],\n",
       "        [1.6300],\n",
       "        [3.0200],\n",
       "        [1.5500]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2805, -0.5674],\n",
       "        [-0.1199, -0.6597]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.3679, 0.1311]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[2].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One step of updating Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2806, -0.5674],\n",
       "        [-0.1199, -0.6597]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.3700, 0.1337]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[2].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Training:\n",
      "Train Loss: 8.7319, Test Loss: 9.0349\n"
     ]
    }
   ],
   "source": [
    "# performance on train  and test sets  before training\n",
    "train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "print(f'Without Training:\\nTrain Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3023],\n",
       "        [-0.3038],\n",
       "        [-0.2824],\n",
       "        [-0.3749],\n",
       "        [-0.2386]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at predictions\n",
    "model(X_train_tensor)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.4841, Test Loss: 0.5649\n",
      "Epoch 2: Train Loss: 0.3706, Test Loss: 0.4299\n",
      "Epoch 3: Train Loss: 0.3623, Test Loss: 0.4170\n",
      "Epoch 4: Train Loss: 0.3572, Test Loss: 0.4150\n",
      "Epoch 5: Train Loss: 0.3536, Test Loss: 0.4099\n",
      "Epoch 6: Train Loss: 0.3509, Test Loss: 0.4078\n",
      "Epoch 7: Train Loss: 0.3492, Test Loss: 0.4094\n",
      "Epoch 8: Train Loss: 0.3472, Test Loss: 0.4068\n",
      "Epoch 9: Train Loss: 0.3461, Test Loss: 0.4048\n",
      "Epoch 10: Train Loss: 0.3453, Test Loss: 0.4047\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "# Execute the training loop\n",
    "for epoch in range(10):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "    # print(epoch,': ', train_loss)\n",
    "    test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2027],\n",
       "        [2.2019],\n",
       "        [2.0986],\n",
       "        [2.4425],\n",
       "        [1.9764]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at predictions\n",
    "model(X_train_tensor)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Train Loss: 3.4033, Test Loss: 3.6030\n",
      "Epoch 200: Train Loss: 1.9215, Test Loss: 2.0787\n",
      "Epoch 300: Train Loss: 1.1776, Test Loss: 1.3044\n",
      "Epoch 400: Train Loss: 0.8047, Test Loss: 0.9098\n",
      "Epoch 500: Train Loss: 0.6182, Test Loss: 0.7079\n",
      "Epoch 600: Train Loss: 0.5249, Test Loss: 0.6037\n",
      "Epoch 700: Train Loss: 0.4779, Test Loss: 0.5489\n",
      "Epoch 800: Train Loss: 0.4537, Test Loss: 0.5194\n",
      "Epoch 900: Train Loss: 0.4408, Test Loss: 0.5026\n",
      "Epoch 1000: Train Loss: 0.4335, Test Loss: 0.4926\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=800, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(1000): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 0.5692, Test Loss: 0.6446\n",
      "Epoch 100: Train Loss: 0.4402, Test Loss: 0.4949\n",
      "Epoch 150: Train Loss: 0.3982, Test Loss: 0.4508\n",
      "Epoch 200: Train Loss: 0.3750, Test Loss: 0.4278\n",
      "Epoch 250: Train Loss: 0.3619, Test Loss: 0.4152\n",
      "Epoch 300: Train Loss: 0.3544, Test Loss: 0.4086\n",
      "Epoch 350: Train Loss: 0.3503, Test Loss: 0.4051\n",
      "Epoch 400: Train Loss: 0.3479, Test Loss: 0.4031\n",
      "Epoch 450: Train Loss: 0.3465, Test Loss: 0.4022\n",
      "Epoch 500: Train Loss: 0.3457, Test Loss: 0.4018\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size= 64, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(500): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all cells till here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly run the model using the new techniques we just looked at that is GD with Momentum and Nesterov Momentum. Let's begin with GD with momentum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've introduced a momentum of 0.9 to the SGD optimizer. Ensure that you add the momentum parameter to the optimizer else the model would simply use the basic SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 0.3634, Test Loss: 0.4161\n",
      "Epoch 100: Train Loss: 0.3498, Test Loss: 0.4048\n",
      "Epoch 150: Train Loss: 0.3479, Test Loss: 0.4036\n",
      "Epoch 200: Train Loss: 0.3466, Test Loss: 0.4026\n",
      "Epoch 250: Train Loss: 0.3455, Test Loss: 0.4019\n",
      "Epoch 300: Train Loss: 0.3446, Test Loss: 0.4014\n",
      "Epoch 350: Train Loss: 0.3439, Test Loss: 0.4012\n",
      "Epoch 400: Train Loss: 0.3433, Test Loss: 0.4006\n",
      "Epoch 450: Train Loss: 0.3427, Test Loss: 0.4008\n",
      "Epoch 500: Train Loss: 0.3423, Test Loss: 0.4008\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size= 64, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(500): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we just observed how with a momentum 0.9 we reached to lower value of loss in a much faster manner! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly run the Nesterov Momentum on our dataset and evaluate the losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is the almost the same as SGD with Momentum, but all you have to do is set the nesterov parameter to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 0.3617, Test Loss: 0.4159\n",
      "Epoch 100: Train Loss: 0.3480, Test Loss: 0.4049\n",
      "Epoch 150: Train Loss: 0.3462, Test Loss: 0.4051\n",
      "Epoch 200: Train Loss: 0.3453, Test Loss: 0.4046\n",
      "Epoch 250: Train Loss: 0.3446, Test Loss: 0.4042\n",
      "Epoch 300: Train Loss: 0.3440, Test Loss: 0.4041\n",
      "Epoch 350: Train Loss: 0.3435, Test Loss: 0.4040\n",
      "Epoch 400: Train Loss: 0.3431, Test Loss: 0.4035\n",
      "Epoch 450: Train Loss: 0.3427, Test Loss: 0.4034\n",
      "Epoch 500: Train Loss: 0.3424, Test Loss: 0.4028\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size= 64, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(500): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss calculations are almost at par with the GD with momentum. Our final train and test loss stand at VALUE and VALUE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things are getting interesting aren’t they. Feel free to revisit the concepts we have covered so far before we move to the next optimizer which is AdaGrad..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's quickly run each of the different optimizers we just looked at starting with AdaGrad. Note that here we've to specify optim.Adagead(model.parameters()) to intialize the model with Adagrad. If you recall, Adagrad adjusts the learning rates of each parameter based on the historical gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.Adagrad(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 4.5387, Test Loss: 4.7627\n",
      "Epoch 100: Train Loss: 2.8909, Test Loss: 3.0783\n",
      "Epoch 150: Train Loss: 1.9554, Test Loss: 2.1167\n",
      "Epoch 200: Train Loss: 1.3775, Test Loss: 1.5177\n",
      "Epoch 250: Train Loss: 1.0100, Test Loss: 1.1330\n",
      "Epoch 300: Train Loss: 0.7737, Test Loss: 0.8825\n",
      "Epoch 350: Train Loss: 0.6219, Test Loss: 0.7193\n",
      "Epoch 400: Train Loss: 0.5249, Test Loss: 0.6129\n",
      "Epoch 450: Train Loss: 0.4632, Test Loss: 0.5438\n",
      "Epoch 500: Train Loss: 0.4242, Test Loss: 0.4989\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size= 64, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(500): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad has given us a high intial loss but the final loss values of VALUE on the train data and VALUE on the test data. Next let's try RMSProp!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMS Prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we did with Adagrad earlier, here we need to use optim.RMSprop to intitialize the model with RMSProp. Even though we are going to use the default parameters, RMSprop also has parameters such as, learning rate, momentum etc, which you can feel free to try out! Let's run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.RMSprop(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 0.3422, Test Loss: 0.4019\n",
      "Epoch 100: Train Loss: 0.3423, Test Loss: 0.3995\n",
      "Epoch 150: Train Loss: 0.3426, Test Loss: 0.4072\n",
      "Epoch 200: Train Loss: 0.3394, Test Loss: 0.4010\n",
      "Epoch 250: Train Loss: 0.3410, Test Loss: 0.3983\n",
      "Epoch 300: Train Loss: 0.3389, Test Loss: 0.4006\n",
      "Epoch 350: Train Loss: 0.3392, Test Loss: 0.4004\n",
      "Epoch 400: Train Loss: 0.3389, Test Loss: 0.4011\n",
      "Epoch 450: Train Loss: 0.3396, Test Loss: 0.4029\n",
      "Epoch 500: Train Loss: 0.3399, Test Loss: 0.4042\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size= 64, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(500): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With RMSProp we have clearly achieved our lowest loss values so far. We have got final loss values of VALUE on the train data and VALUE on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try the Adam Optimizer. For this, we need to use optim.Adam. Take a look at the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialising model weights\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 1.0507, Test Loss: 1.1651\n",
      "Epoch 100: Train Loss: 0.3815, Test Loss: 0.4432\n",
      "Epoch 150: Train Loss: 0.3593, Test Loss: 0.4122\n",
      "Epoch 200: Train Loss: 0.3582, Test Loss: 0.4109\n",
      "Epoch 250: Train Loss: 0.3569, Test Loss: 0.4101\n",
      "Epoch 300: Train Loss: 0.3554, Test Loss: 0.4089\n",
      "Epoch 350: Train Loss: 0.3536, Test Loss: 0.4077\n",
      "Epoch 400: Train Loss: 0.3519, Test Loss: 0.4068\n",
      "Epoch 450: Train Loss: 0.3502, Test Loss: 0.4053\n",
      "Epoch 500: Train Loss: 0.3486, Test Loss: 0.4048\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size= 64, shuffle=True) #800 is the number of samples in train set\n",
    "# Execute the training loop\n",
    "for epoch in range(500): # increasing the epochs for effective training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0: # printing after every 100 epochs\n",
    "        train_loss = criterion(model(X_train_tensor), y_train_tensor).item()\n",
    "        # print(epoch,': ', train_loss)\n",
    "        test_loss = criterion(model(X_test_tensor), y_test_tensor).item()\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adam optimizer too has given us good overall performance. As per our observations, RMSProp is the best of the lot for optimizing the loss in our case. You've now explored a variety of optimization algorithms, each with unique approaches to navigating the complex landscape of neural network training. With this solid foundation of concepts, I’m sure you're well-equipped to apply them in practice. By thoughtfully selecting and implementing the right optimizer, our goal is to fine-tune our model's learning process for better performance and results. I’ll see you in the next video in which we shall build advanced neural models for real world projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
